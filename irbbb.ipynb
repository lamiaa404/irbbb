{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742adba1-54cd-47b6-b445-d82cb2250c64",
   "metadata": {},
   "source": [
    "# Informational Retrieval Project - IR Anthology\n",
    "\n",
    "In this notebook you will write your code, producing the required output for each Milestone.\n",
    "\n",
    "Your notebook must contain 3 types of cells:\n",
    "\n",
    "- (1) Code cells: Cells that contain code snippets, capturing one cohesive fragment of your code.\n",
    "- (2) Corresponding explanation cells: Each code cell must be followed by a text cell containing the **english** explanation of what the corresponding code cell does and what it's purpose is\n",
    "- (3) One reflection cell: One cell at the bottom of the notebook that contains your individual reflection on your process working on this milestones in **english**. It could contain technical problems and how you overcame them, it could contain social problems and how you deal with them (group work is hard!), it could contain explanations of prior skills or knowledge that made certain parts of the task easier for you, etc... (those are just suggestions. Your individual reflections will of course contain different/additional aspects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e766c4",
   "metadata": {},
   "source": [
    "### Step 1: the document collection in .jsonl-format, a form consistent with ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa832d-c0ed-41dd-b4a9-68580c1ae974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Processing\n",
    "\n",
    "import json\n",
    "\n",
    "# 1. Open and read the jsonl file\n",
    "with open(\"/Users/lh/Documents/Projects/ir/data/raw data/ir-anthology-07-11-2021-ss23.jsonl\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 2. Extract the id key and create a new jsonl file\n",
    "with open(\"ir_anthology.jsonl\", 'w') as f:\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        doc_id = data['id']\n",
    "        text = {k: v for k, v in data.items() if k != 'id'}\n",
    "        new_data = {'doc_id': doc_id, 'text': text}\n",
    "        json.dump(new_data, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0440f20e-3c64-44c3-8537-f5065f792f8a",
   "metadata": {},
   "source": [
    "### Explanation for the raw data processing code:\n",
    "\n",
    "1.The code imports the json module, which provides a way to work with JSON data in Python. It then opens a JSON Lines file named \"ir-anthology-07-11-2021-ss23.jsonl\" in read mode and reads all its lines into a list named lines.\n",
    "2.The code creates a new JSON Lines file named \"ir_anthology.jsonl\" in write mode and iterates over each line in lines. For each line, it loads the JSON string into a Python dictionary named data using the json.loads() method. It then extracts the value of the \"id\" key and stores it in a variable named doc_id.\n",
    "Next, the code creates a new dictionary named text that contains all the key-value pairs from the original data dictionary except for the \"id\" key. This is done using a dictionary comprehension that iterates over the items in data and only keeps those that have a key different from \"id\".\n",
    "Finally, the code creates a new dictionary named new_data that contains the \"doc_id\" and \"text\" keys with their corresponding values. It then writes this dictionary as a JSON string to the new file using the json.dump() method, followed by a newline character (\\n) to separate each object in the JSON Lines file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "095961a5",
   "metadata": {},
   "source": [
    "###Step 2: your custom topics for your dataset in TREC XML-format:\n",
    "\n",
    "We chose the topic using text analysis and the content from the lectures that we had so far. We perfomed a text analysis using python and we used chatnoir to check the topics that we came up with. For example we thought of the topic using neural networks in informational retrieval as we found abstracts in the datasets on the topic and then we searched chatnoir using the words \"neural networks in informational retrieval\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcc247-ee10-4db4-ac59-876377b5ce9c",
   "metadata": {},
   "source": [
    "### Add More Explanation cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f614e3-7fe6-4d84-9f25-47fc9b341ccb",
   "metadata": {},
   "source": [
    "### Example Reflection Cell\n",
    "\n",
    "Working on this notebook was difficult, because it is not the real notebook but just an example. My experience in writing notebooks helped speed up the process, for example knowing that there are different types of cells. However, it was difficult to figure our what to write exactly in order to make sure the students understand what they are supposed to do as I could not test it before giving it to the students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9c7e9-8228-4f4f-b942-d460a35eee31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
