{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742adba1-54cd-47b6-b445-d82cb2250c64",
   "metadata": {},
   "source": [
    "# Informational Retrieval Project - IR Anthology\n",
    "\n",
    "In this notebook you will write your code, producing the required output for each Milestone.\n",
    "\n",
    "Your notebook must contain 3 types of cells:\n",
    "\n",
    "- (1) Code cells: Cells that contain code snippets, capturing one cohesive fragment of your code.\n",
    "- (2) Corresponding explanation cells: Each code cell must be followed by a text cell containing the **english** explanation of what the corresponding code cell does and what it's purpose is\n",
    "- (3) One reflection cell: One cell at the bottom of the notebook that contains your individual reflection on your process working on this milestones in **english**. It could contain technical problems and how you overcame them, it could contain social problems and how you deal with them (group work is hard!), it could contain explanations of prior skills or knowledge that made certain parts of the task easier for you, etc... (those are just suggestions. Your individual reflections will of course contain different/additional aspects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e766c4",
   "metadata": {},
   "source": [
    "### Step 1: the document collection in .jsonl-format, a form consistent with ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa832d-c0ed-41dd-b4a9-68580c1ae974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Processing\n",
    "\n",
    "import json\n",
    "\n",
    "# 1. Open and read the jsonl file\n",
    "with open(\"/Users/lh/Documents/Projects/ir/data/raw data/ir-anthology-07-11-2021-ss23.jsonl\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 2. Extract the id key and create a new jsonl file\n",
    "with open(\"ir_anthology.jsonl\", 'w') as f:\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        doc_id = data['id']\n",
    "        text = {k: v for k, v in data.items() if k != 'id'}\n",
    "        new_data = {'doc_id': doc_id, 'text': text}\n",
    "        json.dump(new_data, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0440f20e-3c64-44c3-8537-f5065f792f8a",
   "metadata": {},
   "source": [
    "### Explanation for the raw data processing code:\n",
    "\n",
    "1.The code imports the json module, which provides a way to work with JSON data in Python. It then opens a JSON Lines file named \"ir-anthology-07-11-2021-ss23.jsonl\" in read mode and reads all its lines into a list named lines.\n",
    "2.The code creates a new JSON Lines file named \"ir_anthology.jsonl\" in write mode and iterates over each line in lines. For each line, it loads the JSON string into a Python dictionary named data using the json.loads() method. It then extracts the value of the \"id\" key and stores it in a variable named doc_id.\n",
    "Next, the code creates a new dictionary named text that contains all the key-value pairs from the original data dictionary except for the \"id\" key. This is done using a dictionary comprehension that iterates over the items in data and only keeps those that have a key different from \"id\".\n",
    "Finally, the code creates a new dictionary named new_data that contains the \"doc_id\" and \"text\" keys with their corresponding values. It then writes this dictionary as a JSON string to the new file using the json.dump() method, followed by a newline character (\\n) to separate each object in the JSON Lines file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "095961a5",
   "metadata": {},
   "source": [
    "###Step 2: your custom topics for your dataset in TREC XML-format:\n",
    "\n",
    "We chose the topic using text analysis and the content from the lectures that we had so far. We perfomed a text analysis using python and we used chatnoir to check the topics that we came up with. For example we thought of the topic using neural networks in informational retrieval as we found abstracts in the datasets on the topic and then we searched chatnoir using the words \"neural networks in informational retrieval\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1f614e3-7fe6-4d84-9f25-47fc9b341ccb",
   "metadata": {},
   "source": [
    "### Reflection Cell\n",
    "\n",
       "Working on the first milestone has been a challenging experience for us. While our lab sessions were helpful, we struggled with some aspects of the tutorial and had to spend a significant amount of time researching and seeking clarification. We would have appreciated more detailed explanations in the tutorials, as well as a more tailored task to better suit our needs. While the Q&A session did help to clear up some misunderstandings, it wasn't sufficient in addressing all of our concerns. Despite these difficulties, our teamwork has been crucial in motivating us to push through, especially considering that we had limited prior knowledge in areas such as Python, Docker, and command lines. We are grateful for the support of our tutor and another mentor who helped us find a solution to a particularly challenging problem. In the future, we would welcome more customization of the templates to suit our specific needs, as this would have made our task less daunting.   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
